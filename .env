# Project name
COMPOSE_PROJECT_NAME="stdf-tool"

# Orion
ORION_CONTAINER_NAME = "orion"
ORION_PORT="1026"
ORION_VERSION="3.8.1"

# Core-tool variables
CORE_PORT="5000"

# Whether the core tool should communicate with Orion
FIWARE_ENABLED="true"

# MongoDB variables
MONGO_DATABASE="stdf_db"
MONGO_DB_PORT="27017"
MONGO_DB_VERSION="5.0"
MONGO_ROOT_USERNAME="root"
MONGO_ROOT_PASSWORD="default-password"

# DB related variables of the core tool (names of MongoDB collections)
# Name of collection with meta data on the physical meters / devices (accessed but not managed by the tool)
PHYSICAL_METER_COLLECTION="devices"

# Name of collection with meta data on the virtual meters (managed by the tool)
VIRTUAL_METER_COLLECTION="virtualDevices"

# Name of collection with measurement data of the physical meters defined in PHYSICAL_METER_COLLECTION (accessed but not managed by the tool)
PHYSICAL_METER_MEASUREMENT_COLLECTION="deviceMeasurements"

# Name of the collection that stores the meta data of the trained ML models (managed by the tool)
ML_MODEL_COLLECTION="mlModels"

# Name of the collection that stores statistics regarding the training of ML models (managed by the tool)
STATISTICS_COLLECTION="statistics"


###### Constants to determine holiday dates ######
# Read more here: https://pypi.org/project/holidays/
COUNTRY_CODE="DE"
SUBDIVISION_CODE=


###### Models ######
# The algorithm to use for training if none is specified in the API call
DEFAULT_ALGORITHM="prophet"

# The unit of measurement the consumption data is stored as in the database
# In case of use of an acronym use units accepted in CEFACT code: https://unece.org/trade/uncefact
# MQH corresponds to cubic metre per hour (mÂ³/h)
DEFAULT_MEASUREMENT_UNIT="MQH"

# Weather external features through the weather agent should be used
USE_WEATHER=True

# Maximum number of epochs to train torch-based models (i.e. deep-learning models)
N_EPOCHS_TORCH=50


###### Ray settings for hyperparameter tuning ######
# The number of configurations to test (trials to run) if not specified by the user
RAY_N_MODELS=16

# Maximum number of models that are trained concurrently
RAY_MAX_CONCURRENT=4

# The reduction factor to be applied in Hyperband after a set of models have been trained
# A value of 2 halves the number of configurations to train in the next step and doubles the
# computation budget for each of the remaining configurations
RAY_REDUCTION_FACTOR=2

# Number of CPUs to make available for hyperparameter search
RAY_NUM_CPUS=8

# Number of GPUs to make available for hyperparameter search. In the current implementation,
# the number of GPUs is capped at 1, as the code is not tested for multi-GPU training.
# Note that this environment variable is specified by the docker-compose.yml directly to avoid
# inconsistencies and should not be modified here.
# RAY_NUM_GPUS=0

# Specify a number between 0 and 1 to limit the fraction of the GPU to be used per trial
# that is run concurrently during hyperparameter search
RAY_NUM_GPUS_PER_TRIAL=0.5

# Maximum number of iterations (usually epochs) to train a model
# Difference to N_EPOCHS_TORCH is that RAY_MAX_T is used for trials during hyperparameter tuning
# whereas N_EPOCHS_TORCH is used for training and evaluation of the final model.
RAY_MAX_T=100
